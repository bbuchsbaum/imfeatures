% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/im_features_tv.R
\name{im_features_tv}
\alias{im_features_tv}
\title{Extract Deep Learning Features using thingsvision}
\usage{
im_features_tv(
  impaths,
  model_name,
  source,
  module_name,
  device = "cuda",
  pretrained = TRUE,
  model_parameters = NULL,
  flatten_acts = FALSE,
  batch_size = 32,
  temp_out_dir = tempdir(),
  output_dir = NULL
)
}
\arguments{
\item{impaths}{Character vector. A vector of full file paths to the images
for which features should be extracted. The order of features in the
output will correspond to the order of paths in this vector.
\strong{Note:} Currently, this function assumes all images reside in the
same parent directory. The parent directory is inferred from the first path
in `impaths`. Support for images across multiple directories might require
custom dataloader usage with lower-level `tv_` functions.}

\item{model_name}{Character string. The name of the model architecture (e.g.,
`"resnet50"`, `"clip"`, `"dino-vit-base-p16"`). See the Details section
of \code{\link{tv_get_extractor}} for available models.}

\item{source}{Character string. The source library of the model (e.g.,
`"torchvision"`, `"timm"`, `"ssl"`, `"custom"`). See the Details section
of \code{\link{tv_get_extractor}} for sources.}

\item{module_name}{Character string. The specific layer or module within the
model from which to extract activations. Use
`tv_show_model(tv_get_extractor(model_name, source))` to list available
module names for a given model. Common examples include final layers
like `"avgpool"` or `"fc"` (in ResNets), `"classifier.6"` (in VGG/AlexNet),
`"visual"` (in CLIP), or intermediate layers like `"features.10"` (in VGG).}

\item{device}{Character string. The compute device ("cpu", "cuda", "cuda:0").
Defaults to "cuda".}

\item{pretrained}{Logical. Use pretrained model weights? Defaults to TRUE.}

\item{model_parameters}{Named list (optional). Additional parameters needed for
specific models (e.g., `list(variant = "ViT-B/32")` for CLIP). See
\code{\link{tv_get_extractor}} documentation for details. Defaults to NULL.}

\item{flatten_acts}{Logical. If TRUE, flattens activations from intermediate
layers (e.g., convolutional or transformer layers with spatial dimensions)
into a 2D matrix (n_images x n_features). If FALSE, retains the original
dimensions (e.g., n_images x channels x height x width for CNNs, or
n_images x tokens x dimensions for ViTs, after token extraction).
Defaults to FALSE. Flattening is often necessary for standard machine
learning or statistical analysis comparing images.}

\item{batch_size}{Integer. Number of images to process in each batch. Adjust based
on available GPU memory and model size. Defaults to 32.}

\item{temp_out_dir}{Character string. Path to a directory where a temporary file
listing the order of processed images will be written. This is required
internally by the underlying Python `ImageDataset` to ensure correct
feature ordering. The contents are usually not needed by the end-user.
Defaults to `tempdir()`.}

\item{output_dir}{Character string (optional). If provided, features will be saved
iteratively to this directory in batches (as `.npy` files, since
`output_type="ndarray"` is used internally) instead of being returned directly.
This is useful for very large datasets or models that produce large feature
maps, preventing potential out-of-memory errors in R. If used, the function
returns `NULL` invisibly. Defaults to NULL (features returned in memory).}
}
\value{
An R matrix (if `flatten_acts=TRUE` or the layer is naturally 2D) or
        an R array (if `flatten_acts=FALSE` and the layer has >2 dimensions).
        Rownames corresponding to the base image names (without extension) are
        attempted to be set. Returns `NULL` invisibly if `output_dir` is specified.
}
\description{
This is the primary user-facing function to extract features from images
using a wide variety of models available through the Python `thingsvision` library.
It handles model loading, data preparation, feature extraction, and returns
features in a standard R format.
}
\details{
\strong{Workflow:}
This function performs the following steps internally:
\enumerate{
 \item Calls \code{\link{tv_get_extractor}} to load the specified model via `reticulate`.
 \item Creates a `thingsvision` `ImageDataset` and `DataLoader` to handle image loading,
       preprocessing (using transforms from the extractor), and batching. It writes a
       `file_names.txt` in `temp_out_dir` to preserve order.
 \item Calls \code{\link{tv_extract_features}} to perform the batched feature extraction
       from the specified `module_name`.
 \item Converts the extracted features (typically NumPy arrays from Python) into
       an R matrix or array.
 \item Optionally attempts to add image basenames (without extension) as rownames.
}

\strong{Finding Module Names:}
The `module_name` is critical. To find valid names for your chosen `model_name` and `source`:
\preformatted{
  extractor <- tv_get_extractor("resnet50", "torchvision")
  tv_show_model(extractor) # Prints the model structure
}
Look for meaningful layer names in the output (e.g., `avgpool`, `layer4`, `fc` for ResNet).

\strong{Output Dimensions:}
The dimensions of the returned object depend on the layer (`module_name`) and the
`flatten_acts` parameter:
\itemize{
 \item If `flatten_acts = TRUE`: Returns a 2D matrix (n_images x n_features).
 \item If `flatten_acts = FALSE`:
   \itemize{
     \item For typical final layers (like avgpool, fc, classifier): Often already 2D (n_images x n_features).
     \item For convolutional layers: Returns a 4D array (n_images x channels x height x width).
     \item For transformer layers (after token handling): May return a 3D array (n_images x n_tokens x embedding_dim) or 2D if only CLS token is kept. Check output carefully.
   }
}

\strong{Low-Memory Extraction (`output_dir`):}
When `output_dir` is specified, features for each batch (or group of batches,
controlled by `step_size` in `tv_extract_features`, which this function doesn't expose
directly but uses a default) are saved as separate `.npy` files (e.g., `features_0-32.npy`,
`features_32-64.npy`, ...) in the specified directory. The main R function then returns `NULL`.
You would need to load and combine these files manually after the function completes, e.g., using:
\preformatted{
  feature_files <- list.files(output_dir, pattern = "^features_.*\\.npy$", full.names = TRUE)
  # Ensure correct order if needed, potentially by parsing filenames
  all_features_list <- lapply(sort(feature_files), RcppCNPy::npyLoad)
  all_features <- do.call(rbind, all_features_list) # If they are 2D matrices
}

\strong{Prerequisites:}
Requires a correctly configured Python environment with `thingsvision` and its
dependencies installed. Use \code{\link{install_thingsvision}} to set this up
and configure `reticulate` (e.g., `reticulate::use_condaenv("r-thingsvision")`)
before calling this function.
}
\examples{
\dontrun{
# --- Prerequisites ---
# 1. Install thingsvision Python environment (only needs to be done once)
# install_thingsvision()

# 2. Load library and configure reticulate for the current session
library(imfeatures)
library(reticulate)
tryCatch({
  use_condaenv("r-thingsvision", required = TRUE)
  tv <- import("thingsvision") # Ensure it's loaded
}, error = function(e) {
  message("Python environment 'r-thingsvision' not found or reticulate setup failed.")
  message("Make sure you ran install_thingsvision() and reticulate is configured.")
})

# --- Example Usage ---
# Create some dummy image files for demonstration
image_dir <- file.path(tempdir(), "test_images")
dir.create(image_dir, showWarnings = FALSE)
png(file.path(image_dir, "img1.png")); plot(1:10); dev.off()
png(file.path(image_dir, "img2.png")); plot(rnorm(100)); dev.off()
image_paths <- list.files(image_dir, full.names = TRUE, pattern = "\\\\.png$")

# Example 1: Extract ResNet-18 'avgpool' features (flattened by default layer shape)
features_rn18 <- im_features_tv(
  impaths = image_paths,
  model_name = "resnet18",
  source = "torchvision",
  module_name = "avgpool",
  flatten_acts = TRUE, # Explicitly flatten (though avgpool often is already flat)
  device = "cpu" # Use CPU for this example if no GPU
)
print(dim(features_rn18)) # Should be n_images x 512
print(rownames(features_rn18))

# Example 2: Extract CLIP 'visual' features (ViT-B/32 variant)
# Note: Requires pip install git+https://github.com/openai/CLIP.git in the env
features_clip <- im_features_tv(
   impaths = image_paths,
   model_name = "clip",
   source = "custom",
   module_name = "visual", # The image encoder output
   model_parameters = list(variant = "ViT-B/32"),
   flatten_acts = TRUE, # Usually needed for downstream tasks
   device = "cpu"
)
print(dim(features_clip)) # Should be n_images x 512 for ViT-B/32

# Example 3: Extract intermediate VGG layer (without flattening)
features_vgg_conv <- im_features_tv(
  impaths = image_paths,
  model_name = "vgg16",
  source = "torchvision",
  module_name = "features.10", # An intermediate conv layer
  flatten_acts = FALSE,
  device = "cpu"
)
print(dim(features_vgg_conv)) # Should be 4D: n_images x channels x H x W

# Example 4: Low-memory extraction
low_mem_dir <- file.path(tempdir(), "low_mem_features")
dir.create(low_mem_dir)
result <- im_features_tv(
  impaths = image_paths,
  model_name = "alexnet",
  source = "torchvision",
  module_name = "features.8",
  output_dir = low_mem_dir, # Specify output directory
  flatten_acts = TRUE,
  device = "cpu"
)
print(result) # Should print NULL
print(list.files(low_mem_dir)) # Shows the saved .npy files

# Clean up dummy files
unlink(image_dir, recursive = TRUE)
unlink(low_mem_dir, recursive = TRUE)
}
}
\seealso{
\code{\link{install_thingsvision}}, \code{\link{tv_get_extractor}}, \code{\link{tv_show_model}}, \code{\link{tv_extract_features}}, \code{\link{tv_create_dataset}}, \code{\link{tv_create_dataloader}}
}

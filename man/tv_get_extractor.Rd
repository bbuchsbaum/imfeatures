% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tv_extractor.R
\name{tv_get_extractor}
\alias{tv_get_extractor}
\title{Get a thingsvision extractor object}
\usage{
tv_get_extractor(
  model_name,
  source,
  device = "cuda",
  pretrained = TRUE,
  model_parameters = NULL
)
}
\arguments{
\item{model_name}{Character string. The name of the model.}

\item{source}{Character string. The source library ("torchvision", "timm", etc.).}

\item{device}{Character string. Compute device ("cpu", "cuda", etc.).}

\item{pretrained}{Logical. Use pretrained weights?}

\item{model_parameters}{Named list (optional). Model-specific parameters.}
}
\value{
A reticulate Python object reference to the configured thingsvision extractor.

An R object of class `thingsvision_extractor`.
}
\description{
This function wraps the `get_extractor` function from the Python `thingsvision`
library, allowing you to instantiate a feature extractor for a wide variety
of computer vision models.
}
\details{
The combination of `model_name` and `source` determines which model is loaded.
Here's a guide to common options:

\strong{Sources and Example Models:}

\itemize{
  \item \strong{`source = "torchvision"`}: Accesses models from PyTorch's `torchvision.models`.
    \itemize{
      \item Common `model_name` examples: `"alexnet"`, `"vgg16"`, `"resnet18"`, `"resnet50"`, `"vit_b_16"`
      \item Pretrained weights are typically ImageNet-1k.
      \item `model_parameters`: Can sometimes be used to specify specific weights, e.g., `list(weights = 'IMAGENET1K_V2')` for ResNet50, though `"DEFAULT"` is often sufficient. See torchvision docs for available weights per model.
    }
  \item \strong{`source = "timm"`}: Accesses models from the `pytorch-image-models` library (a very extensive collection).
    \itemize{
      \item Common `model_name` examples: `"efficientnet_b0"`, `"convnext_tiny"`, `"vit_base_patch16_224"`, `"resnet50"`
      \item Find available models via `timm` documentation or `timm.list_models()` in Python.
      \item `model_parameters`: Usually not needed for basic extraction.
    }
  \item \strong{`source = "keras"`}: Accesses models from `tensorflow.keras.applications`.
    \itemize{
      \item Common `model_name` examples: `"VGG16"`, `"ResNet50"`, `"InceptionV3"`, `"EfficientNetB0"` (Note: often capitalized).
      \item Pretrained weights are typically ImageNet-1k.
      \item `model_parameters`: Usually not needed.
    }
  \item \strong{`source = "ssl"`}: Accesses Self-Supervised Learning models.
    \itemize{
      \item ResNet50 variants: `"simclr-rn50"`, `"mocov2-rn50"`, `"barlowtwins-rn50"`, `"vicreg-rn50"`, `"swav-rn50"`, etc.
      \item DINO Vision Transformers: `"dino-vit-small-p8"`, `"dino-vit-base-p16"`, etc.
      \item DINOv2 Vision Transformers: `"dinov2-vit-small-p14"`, `"dinov2-vit-base-p14"`, etc.
      \item MAE Vision Transformers: `"mae-vit-base-p16"`, `"mae-vit-large-p16"`, etc.
      \item `model_parameters`: **Important for ViT models (DINO, MAE)!** Use `list(token_extraction = ...)` to specify how to handle output tokens. Options are:
        \itemize{
          \item `"cls_token"`: Use only the [CLS] token output.
          \item `"avg_pool"`: Average pool the patch tokens (excluding [CLS]).
          \item `"cls_token+avg_pool"`: Concatenate the [CLS] token and the averaged patch tokens.
        }
    }
  \item \strong{`source = "custom"`}: Accesses models specifically packaged or handled by `thingsvision`.
    \itemize{
      \item Official CLIP: `model_name = "clip"`. Requires `model_parameters = list(variant = "ViT-B/32")` or `"RN50"`, etc. Needs `pip install git+https://github.com/openai/CLIP.git` in the Python env.
      \item OpenCLIP: `model_name = "OpenCLIP"`. Requires `model_parameters = list(variant = "ViT-B-32", dataset = "laion2b_s34b_b79k")`, etc. Check OpenCLIP repo for available variant/dataset pairs.
      \item CORnet: `model_name = "cornet_s"`, `"cornet_r"`, `"cornet_rt"`, `"cornet_z"`. Recurrent vision models.
      \item Ecoset Trained Models: `model_name = "Alexnet_ecoset"`, `"VGG16_ecoset"`, `"Resnet50_ecoset"`, `"Inception_ecoset"`. Trained on Ecoset dataset.
      \item Harmonization Models: `model_name = "Harmonization"`. Requires `model_parameters = list(variant = "ViT_B16")` or `"ResNet50"`, etc. Needs extra installation steps (see `install_thingsvision()` docs or thingsvision README).
      \item DreamSim Models: `model_name = "DreamSim"`. Requires `model_parameters = list(variant = "open_clip_vitb32")` or `"clip_vitb32"`, etc. Needs `pip install dreamsim==0.1.2` in the Python env.
      \item Segment Anything (SAM): `model_name = "SegmentAnything"`. Requires `model_parameters = list(variant = "vit_h")` or `"vit_l"`, `"vit_b"`.
      \item Kakaobrain ALIGN: `model_name = "Kakaobrain_Align"`.
    }
}

\strong{`model_parameters` Argument:}
This R `list` is converted to a Python dictionary and passed to the underlying
`thingsvision` or model loading function. It's essential for models where just
the `model_name` isn't enough, like specifying variants (`"ViT-B/32"` for CLIP),
training datasets (`"laion2b_s34b_b79k"` for OpenCLIP), or special extraction
methods (`token_extraction` for DINO/MAE ViTs).

\strong{Return Value:}
The function returns a `reticulate` Python object. This object is a wrapper
around the Python `thingsvision` extractor instance. You will pass this object
to other functions like `tv_extract_features()` or `tv_show_model()`.

\strong{Finding Models:}
For the most up-to-date and comprehensive list of models available through
`torchvision`, `timm`, `keras`, and `ssl`, please refer to their respective
documentations. For `custom` models, refer to the `thingsvision` documentation:
\url{https://vicco-group.github.io/thingsvision/AvailableModels.html}

(Keep the detailed documentation about models/sources/params as before)
...

\strong{Return Value:}
Returns an R object of class `thingsvision_extractor`. This object encapsulates
the underlying Python extractor and provides R methods (like `print`, `extract`,
`align`) for interaction. Use this object with functions designed for it.
}
\examples{
\dontrun{
# Ensure Python env is configured first, e.g. after install_thingsvision()
# reticulate::use_condaenv("r-thingsvision", required = TRUE)

# Example 1: ResNet-18 from Torchvision
extractor_rn18 <- tv_get_extractor(model_name = "resnet18", source = "torchvision")
# tv_show_model(extractor_rn18)

# Example 2: CLIP ViT-B/32 from Custom
extractor_clip <- tv_get_extractor(
   model_name = "clip",
   source = "custom",
   model_parameters = list(variant = "ViT-B/32")
)
# tv_show_model(extractor_clip)

# Example 3: DINO ViT Base/16 from SSL (using cls_token)
extractor_dino <- tv_get_extractor(
   model_name = "dino-vit-base-p16",
   source = "ssl",
   model_parameters = list(token_extraction = "cls_token")
)
# tv_show_model(extractor_dino)

# Example 4: Timm EfficientNet B0
extractor_effnet <- tv_get_extractor(model_name = "efficientnet_b0", source = "timm")
# tv_show_model(extractor_effnet)
}
Get a thingsvision extractor R object

This function instantiates a feature extractor from the Python `thingsvision`
library and wraps it in an R object of class `thingsvision_extractor` for
easier use within R.

\dontrun{
# reticulate::use_condaenv("r-thingsvision", required = TRUE)
extractor_rn18 <- tv_get_extractor(model_name = "resnet18", source = "torchvision", device="cpu")
print(extractor_rn18)
}
}
\seealso{
\code{\link{install_thingsvision}}, \code{\link{tv_extract_features}}, \code{\link{tv_show_model}}

\code{\link{install_thingsvision}}, \code{\link{extract.thingsvision_extractor}}, \code{\link{print.thingsvision_extractor}}, \code{\link{align.thingsvision_extractor}}
}
